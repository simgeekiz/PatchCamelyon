{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D # Conv2D, Input, Flatten, MaxPooling2D, UpSampling2D, concatenate, Cropping2D, Reshape, BatchNormalization\n",
    "from keras.utils import HDF5Matrix\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0516 22:55:24.882844 139756169326720 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/utils/comparams.py:1: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.load_data import load_data\n",
    "from utils.preprocess import DataGenerator\n",
    "from utils.comparams import calculate_auc, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "# data_dir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test_true = load_data(data_dir, purpose='test')#, norm='macenko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes\n",
    "test_id = np.arange(len(x_test))\n",
    "\n",
    "# create a useful dictionary structures\n",
    "partition = {}\n",
    "partition['test'] = test_id\n",
    "    \n",
    "test_labels = {str(i) : y_test_true[i].flatten()[0] for i in test_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'vgg16_model_10k'    \n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 12:09:15.459743 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0512 12:09:15.472865 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0512 12:09:15.488588 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0512 12:09:15.692600 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0512 12:09:15.693038 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0512 12:09:16.254378 140127328833664 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0512 12:09:16.314785 140127328833664 deprecation.py:323] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(network_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg16_10k.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg16_10k_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = np.array(y_test_true).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.array([p[1] for p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9129782979784034\n",
      "tf auc: [0.9129074, 0.9129074]\n"
     ]
    }
   ],
   "source": [
    "calculate_auc(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'vgg19_model_10K'    \n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0512 03:24:42.024550 140039611105408 deprecation.py:323] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(network_filepath, custom_objects=dependencies)\n",
    "# model = model_from_json(open(network_filepath).read())\n",
    "# model.load_weights(os.path.join(os.path.dirname(network_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9086748213509703\n",
      "tf auc: [0.90866363, 0.90866363]\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(y_test_true).flatten()\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19 Predictions with lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'vgg19_model_10K_lrr_0-001'    \n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics, local_variables_initializer\n",
    "from keras.backend import get_session\n",
    "from sklearn.metrics import roc_auc_score as skroc\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = metrics.auc(y_true, y_pred)[1]\n",
    "    get_session().run(local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(network_filepath, custom_objects=dependencies)\n",
    "# model = model_from_json(open(network_filepath).read())\n",
    "# model.load_weights(os.path.join(os.path.dirname(network_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k_lrr_0-001.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k_lrr_0-001_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9062592216085712\n",
      "tf auc: [0.90621924, 0.90621924]\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(y_test_true).flatten()\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19 with learningrate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'vgg19_model_10K_lrr_0.0001'    \n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics, local_variables_initializer\n",
    "from keras.backend import get_session\n",
    "from sklearn.metrics import roc_auc_score as skroc\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = metrics.auc(y_true, y_pred)[1]\n",
    "    get_session().run(local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(network_filepath, custom_objects=dependencies)\n",
    "# model = model_from_json(open(network_filepath).read())\n",
    "# model.load_weights(os.path.join(os.path.dirname(network_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k_lrr_0-0001.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_vgg19_10k_lrr_0-0001_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.8990378027888102\n",
      "tf auc: [0.89904654, 0.89904654]\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(y_test_true).flatten()\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model/lor'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'InceptionResNetV2_best_auc_model'\n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless\n",
    "\n",
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)\n",
    "\n",
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0516 19:06:33.847313 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0516 19:06:33.856495 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0516 19:06:33.858840 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0516 19:06:33.871745 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0516 19:06:34.294844 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0516 19:06:34.469702 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0516 19:06:34.831461 140439981301888 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incres = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = incres.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "y = Dense(2, activation='softmax')(x) # sigmoid instead of softmax to have independent probabilities\n",
    "\n",
    "model = Model(inputs=incres.input, outputs=y)\n",
    "\n",
    "# Train only the top layer\n",
    "for layer in incres.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(network_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0516 19:08:49.355167 140439981301888 deprecation.py:323] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.5856591321427281\n",
      "tf auc: [0.5216076, 0.5216076]\n"
     ]
    }
   ],
   "source": [
    "# whole data normal data predictions inception resnet\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'preds_inception-resnet.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')\n",
    "        \n",
    "        \n",
    "with open(os.path.join(pred_file_dir, 'preds_inception-resnet1_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 Whole data lorenzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model/lor'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'Vgg16_best_auc_model'\n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless\n",
    "\n",
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)\n",
    "\n",
    "from tensorflow import metrics, local_variables_initializer\n",
    "from keras.backend import get_session\n",
    "from sklearn.metrics import roc_auc_score as skroc\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = metrics.auc(y_true, y_pred)[1]\n",
    "    get_session().run(local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "\n",
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = vgg16.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "y = Dense(2, activation='softmax')(x) # sigmoid instead of softmax to have independent probabilities\n",
    "\n",
    "model = Model(inputs=vgg16.input, outputs=y)\n",
    "\n",
    "# Train only the top layer\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(network_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.8615512334406765\n",
      "tf auc: [0.8608843, 0.8608843]\n"
     ]
    }
   ],
   "source": [
    "# whole data normal data predictions vgg16\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'vgg16-whole_data_lore.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')\n",
    "        \n",
    "        \n",
    "with open(os.path.join(pred_file_dir, 'vgg16-whole_data_lore_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xception Whole data lorenzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model/lor'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'Xception_best_auc_model'\n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless\n",
    "\n",
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)\n",
    "\n",
    "from tensorflow import metrics, local_variables_initializer\n",
    "from keras.backend import get_session\n",
    "from sklearn.metrics import roc_auc_score as skroc\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = metrics.auc(y_true, y_pred)[1]\n",
    "    get_session().run(local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "\n",
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = Xception(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x = inc.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "\n",
    "y = Dense(2, activation='softmax')(x) # sigmoid instead of softmax to have independent probabilities\n",
    "\n",
    "model = Model(inputs=inc.input, outputs=y)\n",
    "\n",
    "# Train only the top layer\n",
    "for layer in inc.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(network_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.5039461765191058\n",
      "tf auc: [0.50295633, 0.50295633]\n"
     ]
    }
   ],
   "source": [
    "# whole data normal data predictions xception\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(pred_file_dir, 'xception-whole_data_lore.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction1, prediction2\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[0]) + ',' + str(p[1]) + '\\n')\n",
    "        \n",
    "        \n",
    "with open(os.path.join(pred_file_dir, 'xception-whole_data_lore_sub.csv'), 'w') as f:\n",
    "    f.write(\"case, prediction\\n\")\n",
    "    for i, p in enumerate(preds):\n",
    "        f.write(str(i) + ',' + str(p[1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG19  Changable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to save the best model\n",
    "file_dir = './Model'\n",
    "pred_file_dir = './Preds'\n",
    "model_name = 'vgg16_model_100K_stain_norm'\n",
    "# 'stain_norm_VGG19_model_10K'    \n",
    "network_filepath = os.path.join(file_dir, model_name + '.h5') #_limitless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size.\n",
    "batch_size = 128\n",
    "\n",
    "# Parameters for generators\n",
    "params = {\n",
    "    'dim': (224, 224),\n",
    "    'batch_size': batch_size,\n",
    "    'n_classes': 2,\n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "# Generators\n",
    "test_generator = DataGenerator(partition['test'], x_test, test_labels, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics, local_variables_initializer\n",
    "from keras.backend import get_session\n",
    "from sklearn.metrics import roc_auc_score as skroc\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = metrics.auc(y_true, y_pred)[1]\n",
    "    get_session().run(local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'auc': auc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0516 22:13:48.568025 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0516 22:13:48.582159 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0516 22:13:48.599626 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0516 22:13:48.745999 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0516 22:13:48.746459 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0516 22:13:49.636708 140506729238656 deprecation_wrapper.py:119] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0516 22:13:49.689917 140506729238656 deprecation.py:323] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:808: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0516 22:13:49.800036 140506729238656 deprecation.py:323] From /home/aorus/workspaces/simge/PatchCamelyon/.env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = load_model(network_filepath, custom_objects=dependencies)\n",
    "# model = model_from_json(open(network_filepath).read())\n",
    "# model.load_weights(os.path.join(os.path.dirname(network_filepath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9175704492664041\n",
      "tf auc: [0.9175491, 0.9175491]\n"
     ]
    }
   ],
   "source": [
    "# 100K stain normalized data predictions VGG16\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.92126980290644\n",
      "tf auc: [0.92125595, 0.92125595]\n"
     ]
    }
   ],
   "source": [
    "# 10K normal data predictions VGG16\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9164212156259997\n",
      "tf auc: [0.9163499, 0.9163499]\n"
     ]
    }
   ],
   "source": [
    "# 50K normal data predictions VGG16\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9108792883645189\n",
      "tf auc: [0.91061014, 0.91061014]\n"
     ]
    }
   ],
   "source": [
    "# 100K normal data predictions\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9037854756619346\n",
      "tf auc: [0.90377045, 0.90377045]\n"
     ]
    }
   ],
   "source": [
    "# Stain Norm 10000 predictions\n",
    "true_labels = np.array(y_test_true).flatten()[:len(preds)]\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.9129782979784034\n",
      "tf auc: [0.9129074, 0.9129074]\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(y_test_true).flatten()\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # without beta lr=0.0001 learning rate reduction changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn auc: 0.8974235205864627\n",
      "tf auc: [0.8974233, 0.8974233]\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(y_test_true).flatten()\n",
    "\n",
    "pred_labels = np.array([p[1] for p in preds])\n",
    "\n",
    "calculate_auc(true_labels, pred_labels) # 0.00007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
